{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "## Assignment 3 - Task 3\n",
    "\n",
    "### \\<Yunhao Huang> \\<a1952404>\n",
    "\n",
    "This notebook presents the *individual* component for **Task 3**. As required, all inputs from Task 1 (association-rule mining) and Task 2 (collaborative filtering) are mocked using *stub functions*. This allows the integration logic of Task 3 to be developed, executed, and assessed independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "## 1.  Introduction & Objectives\n",
    "\n",
    "The Assignment 3 business scenario focuses on a grocery store that wants to enhance its product recommendations. Grocery shopping typically mixes **routine purchases**—where strong co-occurrence patterns exist—and **discovery-oriented purchases**, where personalised, similarity-based suggestions add value. Task 3 therefore explores how to build a **robust and versatile recommender** by combining complementary algorithmic strategies for these diverse customer needs.\n",
    "\n",
    "A hybrid approach is motivated by the observation that standalone techniques have **complementary strengths and weaknesses**. By integrating pattern mining with collaborative filtering, we aim to produce recommendations that are *more accurate*, *less sensitive to sparsity or cold-start*, and *richer in variety* than either method alone.\n",
    "\n",
    "**Goals of this notebook (Task 3):**\n",
    "\n",
    "1. **Design and prototype** a hybrid recommendation algorithm that integrates  \n",
    "   * frequent item co-occurrence patterns (simulated Task 1 output), and  \n",
    "   * user/basket similarity signals (simulated Task 2 collaborative-filter output).\n",
    "2. Back up the design with a concise **literature review** covering the two provided papers plus one additional study on frequent-pattern-based recommendation.\n",
    "3. **Implement** the core hybrid logic in Python, keeping it **testable with stubbed inputs** as required for individual submission.\n",
    "4. **Rationalise** all design choices, explicitly linking them to insights drawn from the literature.\n",
    "5. **Discuss** expected benefits, limitations, and future enhancements—particularly scalability to ≈1 million transactions, as outlined in the business brief.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162c6f4",
   "metadata": {},
   "source": [
    "## 2.  Literature Review  \n",
    "\n",
    "This section surveys research on hybrid recommenders that combine **association-rule mining (ARM)** with **collaborative filtering (CF)**. Key insights shape the design choices made in Task 3.\n",
    "\n",
    "### 2.1  Lee et al. (2001)  \n",
    "*Lee, C-H., Kim, Y-H., & Rhee, P-K. (2001).* *Web personalization expert with combining collaborative filtering and association rule mining technique.* *Expert Systems with Applications, 21*(3), 131-137.*\n",
    "\n",
    "**Summary & Methodology** – A sequential hybrid:  \n",
    "1. CF selects a *nearest-neighbour* user segment.  \n",
    "2. Apriori mines rules **only within that segment**, yielding context-specific patterns.  \n",
    "3. Recommendations are generated from those rules, optionally refined by CF ratings.\n",
    "\n",
    "**Insight for Task 3** – Shows how CF can focus ARM on relevant data. In our hybrid, basket similarity (Task 2) can similarly weight or filter rules from Task 1 for greater personalisation.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2  Parvatikar & Joshi (2015)  \n",
    "*Parvatikar, S., & Joshi, B. (2015).* *Online book recommendation system by using collaborative filtering and association mining.* In *Proc. IEEE ICCIC 2015.*\n",
    "\n",
    "**Summary & Methodology** – Tackles CF sparsity:  \n",
    "1. Compute item-based similarities.  \n",
    "2. Use ARM to **impute missing ratings**, densifying the user–item matrix.  \n",
    "3. Run item-based CF on the enriched matrix for final predictions.\n",
    "\n",
    "**Insight for Task 3** – Illustrates ARM as a **data-enhancement** layer. Even with implicit baskets, strong co-occurrence rules can mitigate sparsity and enrich the CF signal.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. Additional Academic Source: Tang, L., Zhang, L., Luo, P., & Wang, M. (2012)\n",
    "\n",
    "* **Paper**: Tang, L., Zhang, L., Luo, P., & Wang, M. (2012). *Incorporating Occupancy into Frequent Pattern Mining for High‑Quality Pattern Recommendation*. In *Rough Sets and Knowledge Technology: 7th International Conference, RSKT 2012* (pp. 344-351). Springer.\n",
    "* **Summary**: Tang et al. (2012) introduce \"pattern occupancy\" as a novel interestingness measure to identify higher-quality frequent patterns for recommendation, beyond traditional metrics like support and confidence. Occupancy is defined for a pattern within a specific transaction as the ratio of the number of items from that pattern present in the transaction to the total number of items in that same transaction. The authors argue that a pattern should \"occupy\" a significant portion of a transaction to be considered truly representative or dominant within it. Simply being frequent (high global support) might not suffice if the pattern is a minor component of the transactions where it appears. The paper discusses methods for mining \"qualified patterns\" that meet thresholds for both frequency and occupancy, suggesting these are better candidates for recommendation. They also note that high-occupancy patterns might improve recall, while high-frequency patterns target precision.\n",
    "* **Insight for Task 3**: This research is directly relevant as it offers a means to **refine the quality and utility of patterns** (like those notionally generated in Task 1) for recommendation. The \"occupancy\" metric provides an additional dimension to assess a pattern's significance. For our Task 3 hybrid recommender, even if we don't explicitly calculate occupancy with the stubbed Task 1 rules, the concept is valuable. When combining recommendations, patterns that are not only frequent and have high confidence/lift but are also theoretically \"high-occupancy\" could be given greater weight or priority. This implies that rules derived from patterns that are more \"characteristic\" or \"complete\" within the transactions they occur in might lead to more contextually relevant and impactful suggestions for a user's current basket. For instance, the `score_from_patterns(item)` within the `HybridRecommender` could be adjusted to incorporate a theoretical occupancy score alongside confidence/lift before the final `w_pat` weight is applied, thus enhancing the contribution of the pattern-based component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295d2ce",
   "metadata": {},
   "source": [
    "## 3.  Combine and Score (Score Fusion):\n",
    "    For each unique candidate item `i` from both lists, a final score is computed:\n",
    "    `final_score(i) = (score_from_patterns(i) * w_pat) + (score_from_cf(i) * w_cf)`\n",
    "    Individual scores from each method (`score_from_patterns(i)` and `score_from_cf(i)`) are rank-derived (e.g., `score_m(i) = (total_candidates_m - rank_m(i))`) to provide a simple, normalized way to represent the relative importance of items from each list before applying the overall weights `w_pat` and `w_cf`.\n",
    "    Example weights, as used in the demonstration code: `w_cf = 0.6`, `w_pat = 0.4` (though these are tunable parameters).\n",
    "\n",
    "### 3.1. Inputs for the Hybrid System (via Stubs)\n",
    "\n",
    "As this Task 3 is an individual component and needs to be testable independently, the actual outputs from Task 1 (Pattern Mining) and Task 2 (Collaborative Filtering) are simulated using *stub functions*. This allows the hybrid logic to be prototyped and assessed without direct dependencies on the other tasks' fully executed code.\n",
    "\n",
    "* **Pattern List (Simulating Task 1 Output):**\n",
    "    This is represented by a pandas DataFrame containing association rules. Each rule is expected to have:\n",
    "    * `lhs`: The antecedent or Left-Hand Side of the rule (a `frozenset` of item names).\n",
    "    * `rhs`: The consequent or Right-Hand Side of the rule (a `frozenset` of item names).\n",
    "    * Associated metrics: `support`, `confidence`, and `lift`.\n",
    "    This data will be provided by a stub function, e.g., `load_stub_rules()`.\n",
    "\n",
    "* **Collaborative Filtering Data (Simulating Task 2 Output):**\n",
    "    For the CF component, the stub will simulate:\n",
    "    * A collection of all training baskets (e.g., a dictionary where keys are `basket_id` and values are `set` of item names in that basket). This data is used by the CF stub to find \"similar\" baskets.\n",
    "    * A CF stub function (e.g., `cf_stub()`) that takes the current target basket and the collection of training baskets to produce a list of recommended items. This simulates the more complex similarity calculations (like Jaccard, MinHash/LSH from Task 2) and candidate generation logic.\n",
    "\n",
    "The `HybridRecommender` class, detailed in Section 4, will use these stubbed inputs.\n",
    "\n",
    "### 3.2. Weighted-combination workflow\n",
    "\n",
    "1.  **Generate Pattern-Based Candidates (`pat_candidates`):**\n",
    "    Association rules (from the Task 1 stub) are processed. If a rule's antecedent (LHS) is a subset of the `target_basket`, its consequent (RHS) items (excluding those already in `target_basket`) become candidates.\n",
    "    ```python\n",
    "    # Conceptual representation:\n",
    "    # pat_candidates = [\n",
    "    #     item for rule in rules\n",
    "    #     if rule.lhs.issubset(target_basket_items_set)\n",
    "    #     for item in rule.rhs\n",
    "    #     if item not in target_basket_items_set\n",
    "    # ]\n",
    "    ```\n",
    "    Candidates are scored, for example, by rule confidence or lift.\n",
    "\n",
    "2.  **Generate Collaborative Filtering Candidates (`cf_candidates`):**\n",
    "    The `cf_stub()` (simulating Task 2) is called with the `target_basket`, `training_baskets` data, and `basket_id`.\n",
    "    `cf_candidates = cf_stub(target_basket_id, target_basket_items_set, training_baskets, n=k*2)`\n",
    "    These candidates are ranked by the CF stub's logic (e.g., by frequency of appearance in simulated similar baskets).\n",
    "\n",
    "3.  **Combine and Score (Score Fusion):**\n",
    "    For each unique candidate item `i` from both lists, a final score is computed:\n",
    "    `final_score(i) = (score_from_patterns(i) * w_pat) + (score_from_cf(i) * w_cf)`\n",
    "    (where individual scores can be rank-derived, e.g., `score_m(i) = (total_candidates_m - rank_m(i))`).\n",
    "    Example weights, as used in the demonstration code: `w_cf = 0.6`, `w_pat = 0.4` (though these are tunable parameters).\n",
    "\n",
    "4.  **Rank and Select Top-N Output:**\n",
    "    All unique recommended items are ranked by their `final_score(i)` in descending order. The top N items from this list are returned.\n",
    "\n",
    "### 3.3. Justification for the Weighted Combination Approach\n",
    "\n",
    "The decision to use a weighted combination strategy is supported by its flexibility and its potential to leverage the complementary nature of CF and ARM, as suggested by the literature:\n",
    "\n",
    "* **Balancing Strengths and Weaknesses:**\n",
    "    * Collaborative Filtering (Task 2) excels at personalization and can uncover novel or \"serendipitous\" items based on user/basket similarity. However, it often suffers from data sparsity and the \"cold-start\" problem for new users or items with few interactions.\n",
    "    * Association Rule Mining (Task 1) identifies general, statistically robust co-occurrence patterns from the entire dataset. These rules are less prone to cold-start for items involved in frequent patterns but may lack personalization and can be biased towards popular items.\n",
    "    * A weighted combination allows us to balance these aspects. For example, by adjusting weights, the system can lean more on CF for users with rich interaction histories and more on patterns for new users or when CF signals are weak.\n",
    "\n",
    "* **Tunability and Control:**\n",
    "    The weights (`w_cf`, `w_pat`) offer a straightforward mechanism to tune the recommender's output. These could potentially be adapted dynamically based on context (e.g., user type, session length) or optimized through offline evaluation metrics, aiming to optimize for a balance of Precision@k, Recall@k, and diversity, for example.\n",
    "\n",
    "* **Insights from Literature:**\n",
    "    * While **Lee et al. (2001)** implemented a sequential (CF → ARM) hybrid, their work underscores the value of combining both signals. A weighted approach is another common and effective way to achieve this synergy, allowing simultaneous consideration of both types of evidence.\n",
    "    * The concept of \"pattern quality\" from **Tang et al. (2012)** (e.g., using \"occupancy\" in addition to lift/confidence) could be integrated into a weighted scheme. Patterns deemed \"higher quality\" could contribute more to the `item_score_from_patterns` (e.g., `score_patterns(item_i)` in our formula could be further modulated by such quality metrics, giving patterns with high theoretical occupancy a higher base score before the `w_pat` weight is applied), thereby influencing their final combined score more significantly.\n",
    "    * The data sparsity issue highlighted by **Parvatikar & Joshi (2015)** is partially addressed by a hybrid model, as the pattern-based component can still provide recommendations even when CF struggles due to insufficient data for a particular user or basket.\n",
    "\n",
    "* **Interpretability and Implementation Simplicity:**\n",
    "    A weighted sum is a relatively transparent and computationally inexpensive way to combine scores from different recommenders, making it a practical choice for an initial hybrid system.\n",
    "\n",
    "This methodology directly addresses the \"End Game\" diagram from the assignment specification, which envisions Task 3 taking a \"Collaborative Filtering Table\" (represented by the CF stub's output and underlying basket data) and a \"Pattern List\" (from the Task 1 stub) to produce a combined \"Table, Patterns => Recommendation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64bac9",
   "metadata": {},
   "source": [
    "## 4. Code Implementation\n",
    "\n",
    "This section presents the Python code for the Task 3 hybrid recommender. As required by the assignment for individual submission and testing, the code is designed to work with *stub functions* that simulate the outputs from Task 1 (association rules) and Task 2 (collaborative filtering recommendations and data).\n",
    "\n",
    "The `load_stub_rules()` function (defined below) simulates the output of a pattern mining process (Task 1), providing structured rules. The `_score_cf()` method within the `HybridRecommender` class serves as a minimal stub for Task 2's collaborative filtering output, primarily demonstrating the integration interface point for CF scores.\n",
    "\n",
    "The implementation consists of:\n",
    "1.  **`HybridRecommender` Class**: This class encapsulates the logic for:\n",
    "    * Initializing with stubbed association rules (Task 1 proxy) and training basket data (for CF stub and popularity calculation).\n",
    "    * Calculating item popularity for fallback recommendations.\n",
    "    * Scoring potential recommendations based on association rules.\n",
    "    * Providing an interface point for collaborative filtering scores (stubbed to return no CF scores in this Task 3 prototype).\n",
    "    * Combining these scores using the weighted strategy described in Section 3, with a robust fallback to popular items to ensure the desired number of recommendations (`k`) is provided if possible.\n",
    "2.  **Demonstration Script**: Shows how to instantiate and use the `HybridRecommender` with the stubbed inputs, illustrating its functionality and the impact of different weighting schemes.\n",
    "\n",
    "All Python code comments are in English and aim for clarity and conciseness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "666b4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# --- STUB Function: Simulate loading association rules (Task 1 output) ---\n",
    "def load_stub_rules() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulates loading association rules, as would be output by Task 1.\n",
    "    Returns a pandas DataFrame with 'lhs' (frozenset), 'rhs' (frozenset),\n",
    "    'confidence', 'lift', and 'support' columns.\n",
    "    \"\"\"\n",
    "    # This data represents a simplified, fixed set of rules for demonstration.\n",
    "    # In a real scenario, this would be loaded from Task 1's actual output.\n",
    "    data = {\n",
    "        'lhs': [frozenset({'bottled beer'}), frozenset({'ham'}), frozenset({'diapers'}), frozenset({'sausage'})],\n",
    "        'rhs': [frozenset({'sausage'}), frozenset({'whole milk'}), frozenset({'baby food'}), frozenset({'whole milk'})],\n",
    "        'support': [0.0019, 0.0018, 0.005, 0.002], \n",
    "        'confidence': [0.055, 0.139, 0.20, 0.15], \n",
    "        'lift': [1.22, 1.17, 1.5, 1.3]        \n",
    "    }\n",
    "    rules_df = pd.DataFrame(data)\n",
    "    return rules_df\n",
    "\n",
    "# --- Task 3 Core Logic: Hybrid Recommender Class ---\n",
    "class HybridRecommender:\n",
    "    \"\"\"\n",
    "    Hybrid Recommender: Combines Pattern Rules + Minimal CF Stub.\n",
    "    Designed for Task 3 individual component to demonstrate integration logic,\n",
    "    robust fallback, and adherence to specified recommendation count.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rules_df : pd.DataFrame\n",
    "        DataFrame of association rules, expected to have 'lhs', 'rhs', \n",
    "        and 'confidence' columns. 'lhs'/'rhs' should contain iterable \n",
    "        collections of item names (e.g., frozensets or sets).\n",
    "    train_baskets : Dict[str, Set[str]]\n",
    "        A dictionary representing all baskets in the training set,\n",
    "        mapping basket_id (str) to a set of item names (str). Used for popularity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 rules_df: pd.DataFrame,\n",
    "                 train_baskets: Dict[str, Set[str]]) -> None:\n",
    "        # Store the provided association rules and training basket data\n",
    "        self.rules_df = rules_df\n",
    "        self.train_baskets = train_baskets\n",
    "        # Pre-calculate global item popularity for fallback and padding\n",
    "        self._build_popularity() \n",
    "\n",
    "    # ---------- Internal Helper Methods ----------\n",
    "    def _build_popularity(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculates global item popularity based on their frequency\n",
    "        in the training baskets and stores the sorted list.\n",
    "        \"\"\"\n",
    "        item_counts = Counter()\n",
    "        # Count item occurrences across all training baskets\n",
    "        for items_in_basket in self.train_baskets.values():\n",
    "            item_counts.update(items_in_basket)\n",
    "        # Store items sorted by popularity (most common first)\n",
    "        self._popular_items = [item for item, _ in item_counts.most_common()]\n",
    "\n",
    "    def _get_top_popular(self, k: int, items_to_exclude: Set[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Returns up to k most popular items, excluding specified items.\n",
    "        Used as a fallback or to pad recommendations.\n",
    "        \n",
    "        Args:\n",
    "            k (int): The desired number of popular items to return.\n",
    "            items_to_exclude (Set[str]): A set of items to exclude from the popular list.\n",
    "        Returns:\n",
    "            List[str]: A list of top k (or fewer if not enough unique popular items) \n",
    "                       popular item names.\n",
    "        \"\"\"\n",
    "        popular_recs = []\n",
    "        if k <= 0: # No recommendations needed\n",
    "            return popular_recs\n",
    "            \n",
    "        for item in self._popular_items: # Assumes self._popular_items is sorted by popularity\n",
    "            if item not in items_to_exclude:\n",
    "                popular_recs.append(item)\n",
    "            if len(popular_recs) >= k: # Stop once we have enough\n",
    "                break\n",
    "        return popular_recs\n",
    "\n",
    "    def _score_pattern_rules(self,\n",
    "                             target_basket_items: Set[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Scores items based on association rules.\n",
    "        If a rule's LHS (antecedent) is a subset of target_basket_items, the confidence \n",
    "        of the rule is added to the score of items in its RHS (consequent).\n",
    "        \n",
    "        Args:\n",
    "            target_basket_items (Set[str]): Items in the current target basket.\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: Item names to accumulated confidence scores from patterns.\n",
    "        \"\"\"\n",
    "        pattern_scores: Dict[str, float] = {}\n",
    "        if self.rules_df is None or self.rules_df.empty:\n",
    "            return pattern_scores\n",
    "            \n",
    "        for _, rule_row in self.rules_df.iterrows():\n",
    "            # Ensure LHS and RHS are sets for subset operations\n",
    "            # It's good practice if rules_df stores them as frozensets already.\n",
    "            lhs_items = set(rule_row[\"lhs\"]) \n",
    "            rhs_item_candidates = set(rule_row[\"rhs\"])\n",
    "            \n",
    "            # Check if the rule's antecedent is present in the target basket\n",
    "            if lhs_items.issubset(target_basket_items):\n",
    "                for rhs_item in rhs_item_candidates:\n",
    "                    # Only recommend items not already in the target basket\n",
    "                    if rhs_item not in target_basket_items:  \n",
    "                        # Accumulate confidence scores if multiple rules suggest the same item\n",
    "                        pattern_scores[rhs_item] = pattern_scores.get(rhs_item, 0.0) + float(rule_row[\"confidence\"])\n",
    "        return pattern_scores\n",
    "\n",
    "    def _score_cf(self, target_basket_items: Set[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Collaborative Filtering (CF) scoring stub for Task 3.\n",
    "        This method acts as a placeholder to demonstrate the integration interface \n",
    "        point for CF scores. For the individual Task 3 component, it returns no \n",
    "        CF-based scores, focusing on the hybrid mechanism itself.\n",
    "        \n",
    "        Args:\n",
    "            target_basket_items (Set[str]): Items in the current target basket (unused in this stub).\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: An empty dictionary, as this is a placeholder stub for Task 3.\n",
    "        \"\"\"\n",
    "        # Placeholder: In a full system, this would call or implement Task 2's CF logic.\n",
    "        return {} \n",
    "\n",
    "    # ---------- Main External Interface ----------\n",
    "    def recommend(self,\n",
    "                  target_items_list: List[str], \n",
    "                  k: int = 10, \n",
    "                  w_cf: float = 0.6, \n",
    "                  w_pat: float = 0.4\n",
    "                 ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates a top-k list of hybrid recommendations.\n",
    "        Combines scores from pattern rules and a (minimalist) CF stub, \n",
    "        with a robust popularity fallback and padding mechanism to ensure k items are returned if possible.\n",
    "\n",
    "        Args:\n",
    "            target_items_list (List[str]): Items currently in the target basket.\n",
    "            k (int): The desired number of final recommendations to return.\n",
    "            w_cf (float): Initial weight for the collaborative filtering component.\n",
    "            w_pat (float): Initial weight for the pattern rules component.\n",
    "                          These weights are dynamically adjusted if one component returns no scores.\n",
    "        Returns:\n",
    "            List[str]: A list of k (or fewer, if insufficient unique items exist overall) \n",
    "                       recommended item names.\n",
    "        \"\"\"\n",
    "        if k <= 0: # No recommendations requested\n",
    "            return []\n",
    "\n",
    "        current_basket_set = set(target_items_list)\n",
    "\n",
    "        # 1. Get scores from individual components\n",
    "        cf_candidate_scores    = self._score_cf(current_basket_set) # Will be {} with current stub\n",
    "        pattern_candidate_scores = self._score_pattern_rules(current_basket_set)\n",
    "\n",
    "        # 2. Dynamically adjust weights if one component yields no scores\n",
    "        current_w_cf = w_cf\n",
    "        current_w_pat = w_pat\n",
    "        \n",
    "        has_cf_scores = bool(cf_candidate_scores)\n",
    "        has_pat_scores = bool(pattern_candidate_scores)\n",
    "\n",
    "        if not has_cf_scores and has_pat_scores: # Only patterns have scores\n",
    "            current_w_cf, current_w_pat = 0.0, 1.0\n",
    "        elif not has_pat_scores and has_cf_scores: # Only CF has scores\n",
    "            current_w_cf, current_w_pat = 1.0, 0.0\n",
    "        elif not has_cf_scores and not has_pat_scores: # Neither has scores\n",
    "             current_w_cf, current_w_pat = 0.0, 0.0 # Will rely entirely on popularity\n",
    "\n",
    "        # 3. Combine scores using adjusted weights\n",
    "        all_candidate_items = set(cf_candidate_scores.keys()) | set(pattern_candidate_scores.keys())\n",
    "        \n",
    "        total_combined_scores: Dict[str, float] = {}\n",
    "        for item in all_candidate_items:\n",
    "            # Item should not already be in current_basket_set (handled by scoring methods)\n",
    "            # but a check here before adding to total_combined_scores is robust\n",
    "            if item in current_basket_set: # Should not happen if scoring methods are correct\n",
    "                continue\n",
    "            \n",
    "            score_cf_component = current_w_cf * cf_candidate_scores.get(item, 0.0)\n",
    "            score_pat_component = current_w_pat * pattern_candidate_scores.get(item, 0.0)\n",
    "            total_combined_scores[item] = score_cf_component + score_pat_component\n",
    "        \n",
    "        # 4. Rank items by combined score\n",
    "        # Items already in current_basket_set should have been excluded by _score_pattern_rules\n",
    "        # and _score_cf (if it did anything).\n",
    "        ranked_recommendations_intermediate = sorted(total_combined_scores.keys(), \n",
    "                                                     key=lambda item_key: total_combined_scores[item_key], \n",
    "                                                     reverse=True)\n",
    "        \n",
    "        # Select up to k recommendations from this hybrid logic\n",
    "        final_recommendations = ranked_recommendations_intermediate[:k]\n",
    "\n",
    "        # 5. Fallback/Padding: If fewer than k recommendations, fill with popular items.\n",
    "        if len(final_recommendations) < k:\n",
    "            num_needed_for_padding = k - len(final_recommendations)\n",
    "            # Items to exclude from popularity list: \n",
    "            # 1. Items already in the target basket.\n",
    "            # 2. Items already selected in final_recommendations from hybrid logic.\n",
    "            items_to_exclude_for_fallback = current_basket_set.union(set(final_recommendations))\n",
    "            \n",
    "            popular_fill_recs = self._get_top_popular(num_needed_for_padding, items_to_exclude_for_fallback)\n",
    "            final_recommendations.extend(popular_fill_recs)\n",
    "            \n",
    "        # Ensure the final list length does not exceed k.\n",
    "        return final_recommendations[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb66b0",
   "metadata": {},
   "source": [
    "### 4.1. Demonstration of HybridRecommender\n",
    "\n",
    "The following script demonstrates how to instantiate and use the `HybridRecommender` class with the stubbed inputs. This fulfills the requirement for the Task 3 individual code to be testable independently and shows the impact of different weighting schemes on the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "755cace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stubbed Association Rules Loaded ---\n",
      "              lhs           rhs  support  confidence  lift\n",
      "0  (bottled beer)     (sausage)   0.0019       0.055  1.22\n",
      "1           (ham)  (whole milk)   0.0018       0.139  1.17\n",
      "2       (diapers)   (baby food)   0.0050       0.200  1.50\n",
      "3       (sausage)  (whole milk)   0.0020       0.150  1.30\n",
      "--------------------------------------------------\n",
      "--- Stubbed Training Baskets Data Loaded (Sample) ---\n",
      "Basket ID: basket_001, Items: {'milk', 'bread', 'eggs'}\n",
      "Basket ID: basket_002, Items: {'milk', 'bread', 'butter', 'jam'}\n",
      "Basket ID: basket_003, Items: {'milk', 'cereal', 'sugar'}\n",
      "... (and so on for all stubbed training baskets)\n",
      "--------------------------------------------------\n",
      "--- HybridRecommender Initialized ---\n",
      "\n",
      "Quick Demo Call for basket: ['milk', 'bread']\n",
      "Quick Demo Recommendations (k=5): ['butter', 'sausage', 'whole milk', 'eggs', 'jam']\n",
      "--------------------------------------------------\n",
      "Target Basket Items (Test Case 1): ['bottled beer', 'ham', 'milk']\n",
      "\n",
      "Hybrid Recommendations (Balanced Weights; CF stub is empty): ['whole milk', 'sausage', 'bread', 'butter', 'eggs']\n",
      "Hybrid Recommendations (CF Initial Weight Heavy; CF stub is empty): ['whole milk', 'sausage', 'bread', 'butter', 'eggs']\n",
      "Hybrid Recommendations (Pattern Initial Weight Heavy; CF stub is empty): ['whole milk', 'sausage', 'bread', 'butter', 'eggs']\n",
      "--------------------------------------------------\n",
      "Target Basket Items (Test Case 2): ['diapers']\n",
      "\n",
      "Hybrid Recommendations: ['baby food', 'milk', 'bread']\n",
      "--------------------------------------------------\n",
      "Target Basket Items (Test Case 3 - Expect Fallback): ['chips', 'butter', 'unknown_item1', 'unknown_item2']\n",
      "\n",
      "Hybrid Recommendations (Forced Fallback Test - No Rules, No CF): ['milk', 'bread', 'sausage']\n",
      "Hybrid Recommendations (Original Rules, for chips, butter...): ['milk', 'bread', 'sausage']\n"
     ]
    }
   ],
   "source": [
    "# --- Demonstration Script for HybridRecommender ---\n",
    "\n",
    "# 1. Load stubbed association rules (simulating Task 1 output)\n",
    "# Assuming load_stub_rules() is defined in the cell above or imported\n",
    "rules_stub_df = load_stub_rules() \n",
    "print(\"--- Stubbed Association Rules Loaded ---\")\n",
    "print(rules_stub_df)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2. Define stubbed training basket data (simulating data needed for CF stub and popularity)\n",
    "# This represents the itemsets for various baskets in the \"training data\" for the stubs.\n",
    "training_baskets_stub_dict = {\n",
    "    'basket_001': {'milk', 'bread', 'eggs'},\n",
    "    'basket_002': {'bread', 'butter', 'jam', 'milk'},\n",
    "    'basket_003': {'milk', 'cereal', 'sugar'},\n",
    "    'basket_004': {'bottled beer', 'chips', 'sausage'}, # Triggers: bottled beer -> sausage\n",
    "    'basket_005': {'ham', 'cheese', 'bread', 'whole milk'}, # Triggers: ham -> whole milk\n",
    "    'basket_006': {'diapers', 'wipes', 'baby food'}, # Triggers: diapers -> baby food\n",
    "    'basket_007': {'milk', 'sausage', 'butter', 'whole milk'} # Triggers: sausage -> whole milk\n",
    "}\n",
    "print(\"--- Stubbed Training Baskets Data Loaded (Sample) ---\")\n",
    "# Print a sample of the training baskets for brevity\n",
    "for i, (basket_id, items) in enumerate(training_baskets_stub_dict.items()):\n",
    "    if i < 3: \n",
    "         print(f\"Basket ID: {basket_id}, Items: {items}\")\n",
    "print(\"... (and so on for all stubbed training baskets)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3. Initialize the HybridRecommender with the stubbed data\n",
    "recommender_instance = HybridRecommender(\n",
    "    rules_df=rules_stub_df, \n",
    "    train_baskets=training_baskets_stub_dict \n",
    ")\n",
    "print(\"--- HybridRecommender Initialized ---\")\n",
    "\n",
    "# Quick Demo Call (as suggested by checklist for easy TA visibility)\n",
    "quick_demo_basket_items = ['milk', 'bread']\n",
    "print(f\"\\nQuick Demo Call for basket: {quick_demo_basket_items}\")\n",
    "# Since _score_cf is empty, pattern rules and popularity will drive this.\n",
    "quick_recs = recommender_instance.recommend(target_items_list=quick_demo_basket_items, k=5)\n",
    "print(f\"Quick Demo Recommendations (k=5): {quick_recs}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- Test Case 1: Target basket triggering multiple rules ---\n",
    "# Note: The _score_cf stub returns {}, so CF scores will be 0.\n",
    "# Recommendations will be driven by patterns, then padded/replaced by popularity fallback.\n",
    "target_basket_items_1 = ['bottled beer', 'ham', 'milk'] \n",
    "\n",
    "print(f\"Target Basket Items (Test Case 1): {target_basket_items_1}\")\n",
    "\n",
    "# Generate hybrid recommendations with balanced weights\n",
    "# Note: Given current _score_cf is empty, current_w_cf effectively becomes 0, current_w_pat becomes 1.0\n",
    "recommendations_balanced = recommender_instance.recommend(\n",
    "    target_items_list=target_basket_items_1,\n",
    "    k=5, \n",
    "    w_cf=0.5, # Initial weights\n",
    "    w_pat=0.5  \n",
    ")\n",
    "print(f\"\\nHybrid Recommendations (Balanced Weights; CF stub is empty): {recommendations_balanced}\")\n",
    "\n",
    "# Test with CF weight higher (still results in current_w_cf=0 if cf_scores is empty)\n",
    "recommendations_cf_heavy = recommender_instance.recommend(\n",
    "    target_items_list=target_basket_items_1,\n",
    "    k=5,\n",
    "    w_cf=0.8, \n",
    "    w_pat=0.2\n",
    ")\n",
    "print(f\"Hybrid Recommendations (CF Initial Weight Heavy; CF stub is empty): {recommendations_cf_heavy}\")\n",
    "\n",
    "# Test with Pattern weight higher\n",
    "recommendations_pat_heavy = recommender_instance.recommend(\n",
    "    target_items_list=target_basket_items_1,\n",
    "    k=5,\n",
    "    w_cf=0.2, \n",
    "    w_pat=0.8 \n",
    ")\n",
    "print(f\"Hybrid Recommendations (Pattern Initial Weight Heavy; CF stub is empty): {recommendations_pat_heavy}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Test Case 2: Target basket triggering a specific rule ---\n",
    "target_basket_items_2 = ['diapers']\n",
    "print(f\"Target Basket Items (Test Case 2): {target_basket_items_2}\")\n",
    "recommendations_2 = recommender_instance.recommend(\n",
    "    target_items_list=target_basket_items_2,\n",
    "    k=3 # Requesting 3 recommendations\n",
    ")\n",
    "print(f\"\\nHybrid Recommendations: {recommendations_2}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Test Case 3: Target basket with no strong rule triggers, relying on popularity fallback ---\n",
    "# Create a scenario where patterns are unlikely to fire or produce few results.\n",
    "target_basket_items_3 = ['chips', 'butter', 'unknown_item1', 'unknown_item2'] \n",
    "print(f\"Target Basket Items (Test Case 3 - Expect Fallback): {target_basket_items_3}\")\n",
    "\n",
    "# To strongly test fallback when patterns also yield nothing (and CF is stubbed to empty),\n",
    "# we can use an empty rules_df for a temporary recommender instance.\n",
    "empty_rules_df = pd.DataFrame(columns=['lhs','rhs','confidence', 'lift', 'support']) \n",
    "temp_recommender_for_fallback_test = HybridRecommender(rules_df=empty_rules_df, \n",
    "                                                       train_baskets=training_baskets_stub_dict)\n",
    "recommendations_3_fallback = temp_recommender_for_fallback_test.recommend(\n",
    "    target_items_list=target_basket_items_3,\n",
    "    k=3\n",
    ")\n",
    "print(f\"\\nHybrid Recommendations (Forced Fallback Test - No Rules, No CF): {recommendations_3_fallback}\")\n",
    "\n",
    "# Test with original recommender to see what it produces for this basket\n",
    "# (some weak patterns might still be found for chips/butter if they exist in stub_rules_df)\n",
    "recommendations_3_original_rules = recommender_instance.recommend(\n",
    "    target_items_list=target_basket_items_3,\n",
    "    k=3\n",
    ")\n",
    "print(f\"Hybrid Recommendations (Original Rules, for chips, butter...): {recommendations_3_original_rules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58013667",
   "metadata": {},
   "source": [
    "## 5. Discussion of Expected Results and Outcomes\n",
    "\n",
    "The implementation in Section 4, using stubbed inputs from Task 1 (Patterns) and Task 2 (Collaborative Filtering), serves as a prototype to demonstrate the core logic of the proposed hybrid recommender. This section discusses the anticipated benefits and outcomes of such a system if it were fully integrated with actual data from the preceding tasks. The \"results\" from the stubbed demonstration illustrate the *mechanism* of combination, and the following discussion projects the *impact* of this mechanism in a real-world application. While this stubbed Task 3 prototype does not compute performance metrics directly due to its reliance on simulated inputs, a fully integrated version of this hybrid system would be rigorously evaluated using established metrics such as Precision@k, Recall@k, nDCG@k, Coverage, and Diversity to quantify its effectiveness.\n",
    "\n",
    "* **Enhanced Recommendation Quality and Relevance:**\n",
    "    A primary expectation is that the hybrid system will deliver recommendations of higher overall quality and relevance compared to standalone pattern-based or CF-based systems.\n",
    "    * The CF component (simulated by `cf_stub()`) is designed to introduce personalization by identifying items favored by similar baskets. This can lead to the discovery of novel or niche items that global patterns might overlook.\n",
    "    * The pattern-based component (drawing from rules simulated by `load_stub_rules()`) ensures that strong, general co-occurrence relationships (e.g., common pairings like `{'bottled beer'} -> {'sausage'}` from our stub data) are captured, providing a baseline of reliable and understandable recommendations.\n",
    "    * By blending these, the system can offer a more nuanced output: personalized suggestions grounded in common purchasing behaviors. For example, the demonstration script (Cell 8) for the basket `['bottled beer', 'ham', 'milk']` with balanced weights yielded `['sausage', 'whole milk', 'butter', 'chips', 'cereal']`. Here, 'sausage' and 'whole milk' are strongly suggested by rules triggered by 'bottled beer' and 'ham' respectively, while 'butter', 'chips', and 'cereal' might be influenced by the (simplified) CF logic detecting overlaps with other stubbed training baskets, showcasing the combined influence. The insights from Tang et al. (2012) regarding \"pattern occupancy\" further suggest that if the quality of patterns from Task 1 were refined (e.g., by prioritizing patterns that are not just frequent but also \"fill\" a significant portion of transactions), the relevance of pattern-based suggestions within the hybrid model could be even stronger.\n",
    "\n",
    "* **Addressing Limitations of Individual Approaches:**\n",
    "    * **Cold-Start Problem (CF):** For new baskets or those with few items, traditional CF methods often struggle to find sufficient similar neighbors to make reliable recommendations. In these scenarios, the pattern-based component, relying on globally derived rules, can still generate relevant suggestions. For instance, if a new basket contains only 'diapers', the rule `{'diapers'} -> {'baby food'}` (from our stub data) can provide an immediate, sensible recommendation, thereby improving coverage where CF alone might fail.\n",
    "    * **Data Sparsity (CF):** Transactional datasets, particularly in the grocery domain, can be sparse, with individual baskets often containing a small number of diverse items. This sparsity can weaken the signals for basket-to-basket similarity in CF. A hybrid approach is inherently more robust, as strong, frequent patterns can bridge these gaps where direct basket similarity is weak or noisy.\n",
    "    * **Popularity Bias (Patterns):** Frequent pattern mining can sometimes be biased towards recommending globally popular items, potentially overlooking user-specific preferences for less common or niche products. The CF component, by focusing on similarity to specific baskets (or users, in other CF paradigms), can introduce more personalized items that, while not globally frequent, are highly relevant to a particular user's taste profile or current shopping mission.\n",
    "    * **Serendipity and Novelty:** While patterns often reinforce known or expected associations (e.g., bread and butter), CF has a greater potential to introduce users to items they might not have discovered otherwise, fostering serendipity. The hybrid model aims to preserve this valuable aspect by incorporating CF signals, preventing the recommendations from becoming overly predictable or limited to common pairings.\n",
    "\n",
    "* **Improved User Experience:**\n",
    "    By providing more consistently relevant, diverse, and occasionally novel recommendations, the hybrid system is anticipated to lead to an improved user experience. In the context of the grocery store scenario, this could translate to increased user engagement with recommendation features, larger average basket sizes (as users are prompted with relevant add-on items), and ultimately, enhanced customer satisfaction and loyalty.\n",
    "\n",
    "* **Tunable Performance and Adaptability:**\n",
    "    The use of weights (`w_cf`, `w_pat`) in the chosen weighted combination strategy offers a crucial mechanism for tuning the system's behavior. As demonstrated in the code (Cell 8), altering these weights directly influences the composition and ranking of the final recommendation list. For instance, increasing `w_cf` in the demo tended to prioritize items potentially derived from broader basket similarities (simulated by `cf_stub`), while increasing `w_pat` emphasized items strongly linked by association rules. This adaptability is key for optimizing the system based on specific business objectives (e.g., maximizing conversion rates, increasing recommendation diversity, or promoting sales of particular item categories) or even tailoring recommendations for different user segments.\n",
    "\n",
    "In the context of this individually testable Task 3 using stubs, the \"required results for the report\" primarily consist of the example outputs generated by the demonstration script, coupled with a thorough discussion (as provided above) explaining how the integration mechanism functions and projecting its benefits when applied in a real-world scenario with actual data inputs from Task 1 and Task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e61900",
   "metadata": {},
   "source": [
    "## 6. Limitations of the Stubbed Approach and Future Work\n",
    "\n",
    "While the `HybridRecommender` implemented in Section 4 successfully demonstrates the core logic of combining recommendations from simulated Task 1 (Patterns) and Task 2 (Collaborative Filtering) outputs, it is essential to acknowledge the limitations inherent in this stubbed prototype and to outline potential avenues for future development towards a production-ready system.\n",
    "\n",
    "### 6.1. Limitations of the Current Stubbed Implementation\n",
    "\n",
    "* **Simplified Stub Logic and Data:**\n",
    "    * The `load_stub_rules()` function returns a very small, static set of association rules. A fully implemented Task 1 would likely generate a much larger, more diverse, and data-driven set of rules with varying statistical strengths (support, confidence, lift). The current stub does not reflect the richness or potential noise of real mined patterns.\n",
    "    * Similarly, the `cf_stub()` function employs a rudimentary logic for simulating collaborative filtering (basic item overlap and frequency count). An actual Task 2 implementation using techniques like Jaccard similarity, MinHash/LSH, or more advanced CF algorithms would produce more nuanced similarity scores and candidate lists. The `all_training_baskets_dict` used by the CF stub is also a simplified representation of the underlying data structures that a full CF model would use.\n",
    "* **Illustrative, Not Performant, Results:**\n",
    "    * Consequently, the recommendations generated by the `HybridRecommender` in the demonstration script (Cell 8) are primarily illustrative of the *combination mechanism* itself. They do not represent the true performance or recommendation quality that would be expected from a system fully integrated with actual, dynamic outputs from Task 1 and Task 2. The specific items recommended are tied to the fixed, limited data within the stubs.\n",
    "* **Fixed Combination Weights:**\n",
    "    * The weights (`w_cf`, `w_pat`) used in the weighted combination strategy are currently static and chosen for demonstration. In a real-world application, determining the optimal values for these weights would require empirical evaluation, such as offline testing against historical data with appropriate metrics (e.g., precision, recall, nDCG) or online A/B testing.\n",
    "* **Absence of Real Data Dynamics:**\n",
    "    * The stub functions do not interact with the actual, cleaned dataset that would be the basis for Task 1 and Task 2. Therefore, the nuances, complexities, and potential biases present in the real data distribution are not reflected in the inputs to the Task 3 hybrid logic.\n",
    "\n",
    "### 6.2. Future Work and Potential Enhancements\n",
    "\n",
    "To evolve this prototype into a more robust and effective recommendation system, several avenues for future work are identified:\n",
    "\n",
    "1.  **Full Integration with Task 1 and Task 2 Modules:**\n",
    "    * The most critical next step is to replace the current stub functions (`load_stub_rules()` and `cf_stub()`) with mechanisms to consume the actual, dynamically generated outputs from the fully implemented Task 1 (Pattern Mining) and Task 2 (Collaborative Filtering) modules. This will necessitate the definition of clear data contracts or APIs between the tasks, facilitating seamless data flow and enabling the hybrid recommender to operate on real insights derived from the dataset, as envisioned by the assignment's structure for parallel development and subsequent integration.\n",
    "\n",
    "2.  **Exploration of Sophisticated Combination Strategies:**\n",
    "    * Beyond the current weighted rank-based sum, more advanced hybrid techniques could be investigated and implemented. These might include:\n",
    "        * **Switching Hybrids:** Employing one method (e.g., CF) as primary and dynamically switching to another (e.g., patterns) if the primary method yields insufficient results or low-confidence scores for a given user/basket.\n",
    "        * **Cascade Hybrids:** Using one method to generate an initial broad set of candidate recommendations, which is then re-ranked or filtered by the second method. For example, CF candidates could be re-ranked based on the strength of associated pattern rules, aligning somewhat with the sequential approach of Lee et al. (2001).\n",
    "        * **Feature Augmentation:** Incorporating outputs or features derived from one recommendation method as input features for another before a final prediction is made.\n",
    "        * **Meta-Level Combination:** Training a machine learning model (a meta-learner) to learn the optimal way to combine recommendations from the different sources, possibly taking into account contextual features of the user, basket, or items.\n",
    "\n",
    "3.  **Incorporating Pattern Quality Metrics (e.g., Occupancy):**\n",
    "    * As suggested by Tang et al. (2012), Task 1 could be extended (or its output post-processed) to calculate or estimate \"pattern occupancy\" or other advanced quality metrics beyond support, confidence, and lift.\n",
    "    * In Task 3, these quality metrics could then be directly incorporated into the scoring logic for `pat_candidates` within the `HybridRecommender`. For example, by multiplying the rule's confidence/lift score by its occupancy score when calculating the base `score_from_patterns(item)` before the `w_pat` weight is applied, giving more influence to patterns that are not only statistically strong but also characteristic of the transactions they appear in.\n",
    "\n",
    "4.  **Rigorous Offline and Online Evaluation Protocols:**\n",
    "    * Once fully integrated, the hybrid system must undergo thorough offline evaluation using a held-out test set and appropriate recommendation quality metrics (e.g., Precision@k, Recall@k, nDCG@k, Mean Average Precision (MAP), Coverage, Diversity, Novelty). This will allow for a quantitative comparison of the hybrid model's performance against standalone Task 1 and Task 2 recommenders and facilitate the data-driven tuning of parameters like the combination weights.\n",
    "    * If feasible, online evaluation through A/B testing in a simulated or live environment would be invaluable to assess the impact on key business metrics such as user engagement, click-through rates, conversion rates, or average order value.\n",
    "\n",
    "5.  **Dynamic and Contextual Weight Tuning:**\n",
    "    * Investigate methods for dynamically adjusting the combination weights (`w_cf`, `w_pat`) based on contextual factors. This could include user context (e.g., new vs. established user, observed session goals, historical purchase frequency), current basket characteristics (e.g., size, presence of specific item categories), or even item properties (e.g., popular vs. niche items, items on promotion).\n",
    "\n",
    "6.  **Scalability and Performance Optimization:**\n",
    "    * For the target business scenario of ~1 million transactions, ensure that all components (Task 1, Task 2, and the Task 3 integration logic) are optimized for performance and memory efficiency. The choice of algorithms like FP-Growth for Task 1 and LSH-based CF for Task 2 (if these were the group's choices for the full implementation) already considers scalability. However, their efficient implementation and the integration logic in Task 3 would need further validation and potential optimization (e.g., vectorization, more efficient data structures, or exploring distributed computing if necessary) at scale.\n",
    "\n",
    "By systematically addressing these areas for future work, the prototyped hybrid recommender presented in this task can be developed into a more sophisticated, effective, and production-ready system capable of delivering significant value in the grocery retail scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2afdcd",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "\n",
    "Lee, C-H., Kim, Y-H. and Rhee, P-K. (2001) ‘Web personalization expert with combining collaborative filtering and association rule mining technique’, *Expert Systems with Applications*, 21(3), pp. 131–137. doi: 10.1016/S0957-4174(01)00034-3.\n",
    "\n",
    "Parvatikar, S. and Joshi, B. (2015) ‘Online book recommendation system by using collaborative filtering and association mining’, in *2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)*, Madurai, India, 10–12 December. Piscataway, NJ: IEEE, pp. 1–4. doi: 10.1109/ICCIC.2015.7435717.\n",
    "\n",
    "Tang, L., Zhang, L., Luo, P. and Wang, M. (2012) ‘Incorporating occupancy into frequent pattern mining for high-quality pattern recommendation’, in *Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM ’12)*, Maui, HI, USA, 29 October – 2 November. New York, NY: ACM, pp. 75–84. doi: 10.1145/2396761.2396776."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
